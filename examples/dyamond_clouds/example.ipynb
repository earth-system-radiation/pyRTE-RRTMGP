{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pyrte_rrtmgp.external_data_helpers import download_dyamond2_data\n",
    "\n",
    "# Download the data\n",
    "downloaded_files = download_dyamond2_data(\n",
    "    datetime(2020, 2, 1, 9),\n",
    "    compute_gas_optics=False,\n",
    "    data_dir=\"GEOS-DYAMOND2-data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganize the data to improve the processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "from pyrte_rrtmgp.constants import HELMERT1\n",
    "\n",
    "nlev = 181\n",
    "min_lev_ice = 78\n",
    "\n",
    "# Load the global dataset\n",
    "atmosphere = (\n",
    "    xr.open_mfdataset(\n",
    "        \"GEOS-DYAMOND2-data/*inst_01hr_3d_*.nc4\",\n",
    "        drop_variables=[\n",
    "            \"anchor\",\n",
    "            \"cubed_sphere\",\n",
    "            \"orientation\",\n",
    "            \"contacts\",\n",
    "            \"corner_lats\",\n",
    "            \"corner_lons\",\n",
    "        ],\n",
    "    )\n",
    "    .isel(lev=slice(min_lev_ice, nlev))\n",
    "    .rename({\"lev\": \"layer\"})\n",
    "    .chunk({\"Xdim\": 2880, \"Ydim\": 72, \"nf\": 1, \"layer\": -1})\n",
    ")\n",
    "\n",
    "# Need to convert LWP/IWP to g/m2 and rel/rei to microns\n",
    "atmosphere[\"lwp\"] = (atmosphere[\"DELP\"] * atmosphere[\"QL\"]) * 1000 / HELMERT1\n",
    "atmosphere[\"iwp\"] = (atmosphere[\"DELP\"] * atmosphere[\"QI\"]) * 1000 / HELMERT1\n",
    "atmosphere[\"rel\"] = atmosphere[\"RL\"] * 1e6\n",
    "atmosphere[\"rei\"] = atmosphere[\"RI\"] * 1e6\n",
    "\n",
    "needed_vars = [\"lwp\", \"iwp\", \"rel\", \"rei\"]\n",
    "\n",
    "atmosphere[needed_vars].to_netcdf(\n",
    "    \"atmosphere.nc\",\n",
    "    encoding={var: {\"zlib\": True, \"complevel\": 5} for var in needed_vars},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the cloud optics\n",
    "\n",
    "For avoiding memory issues please use dask version 2025.3.0 or higher. A [fix](https://docs.dask.org/en/stable/changelog.html#v2025-3-0) for the apply_ufunc was included in it that solve the memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from pyrte_rrtmgp import rrtmgp_cloud_optics\n",
    "from pyrte_rrtmgp.data_types import CloudOpticsFiles\n",
    "\n",
    "atmosphere = xr.open_dataset(\n",
    "    \"atmosphere.nc\", chunks={\"Ydim\": 72, \"nf\": 1, \"Xdim\": -1, \"time\": 1}\n",
    ")\n",
    "\n",
    "# Load cloud optics (this object is relatively small and will be serialized to workers)\n",
    "cloud_optics_lw = rrtmgp_cloud_optics.load_cloud_optics(\n",
    "    cloud_optics_file=CloudOpticsFiles.LW_BND\n",
    ")\n",
    "\n",
    "# Define the function to be applied to each chunk\n",
    "\n",
    "\n",
    "def process_chunk(atm_chunk):\n",
    "    from pyrte_rrtmgp import rrtmgp_cloud_optics\n",
    "\n",
    "    tau_chunk_ds = cloud_optics_lw.compute_cloud_optics(\n",
    "        atm_chunk, problem_type=\"absorption\", add_to_input=False\n",
    "    )\n",
    "    # Assuming the relevant variable is the first one if it's a Dataset, or just use the DataArray\n",
    "    if isinstance(tau_chunk_ds, xr.Dataset):\n",
    "        # Infer the name of the output variable if possible, or assume a default/first one\n",
    "        output_var_name = list(tau_chunk_ds.data_vars.keys())[\n",
    "            0\n",
    "        ]  # Adjust if needed\n",
    "        tau_chunk = tau_chunk_ds[output_var_name]\n",
    "    else:\n",
    "        tau_chunk = tau_chunk_ds\n",
    "\n",
    "    # Aggregate over 'bnd' and 'layer' dimensions\n",
    "    # Ensure the output is float32\n",
    "    tau_agg_chunk = tau_chunk.sum(dim=[\"bnd\", \"layer\"], skipna=True).astype(\n",
    "        np.float32\n",
    "    )\n",
    "    # The result should be a DataArray with dimensions (Ydim_chunk, Xdim_chunk)\n",
    "    tau_agg_chunk[\"lats\"] = atm_chunk[\"lats\"]\n",
    "    return tau_agg_chunk\n",
    "\n",
    "\n",
    "dims_to_remove = [\"layer\"]\n",
    "\n",
    "# Determine dimensions, coordinates, shape, and chunks for the template\n",
    "kept_coords = [\n",
    "    coord for coord in atmosphere.coords.keys() if coord not in dims_to_remove\n",
    "]\n",
    "template_coords = {\n",
    "    coord: atmosphere.coords[coord]\n",
    "    for coord in kept_coords\n",
    "    if coord in atmosphere.coords\n",
    "}\n",
    "kept_dims = [dim for dim in atmosphere.dims if dim in kept_coords]\n",
    "\n",
    "# Create a template array like lwp but without the level dimension\n",
    "dask_data = xr.full_like(\n",
    "    atmosphere[\"lwp\"].isel(layer=0, drop=True), np.nan, dtype=np.float32\n",
    ")\n",
    "\n",
    "# Create template DataArray with dask array\n",
    "template_agg = xr.DataArray(\n",
    "    data=dask_data,\n",
    "    dims=kept_dims,\n",
    "    coords=template_coords,\n",
    "    name=\"aggregated_tau\",\n",
    ")\n",
    "\n",
    "# --- Apply the function chunk-wise using map_blocks ---\n",
    "tau_agg = xr.map_blocks(\n",
    "    process_chunk,\n",
    "    atmosphere,  # Input Dataset (chunked)\n",
    "    template=template_agg,  # Provide the template with properly chunked dask arrays\n",
    ")\n",
    "\n",
    "with ProgressBar():\n",
    "    result = tau_agg.compute(scheduler=\"multiprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from pyrte_rrtmgp import rrtmgp_cloud_optics\n",
    "from pyrte_rrtmgp.data_types import CloudOpticsFiles\n",
    "\n",
    "atmosphere = xr.open_dataset(\n",
    "    \"atmosphere.nc\",\n",
    "    chunks={\"Ydim\": 36, \"nf\": 1, \"Xdim\": -1, \"time\": 1, \"layer\": -1},\n",
    ")\n",
    "\n",
    "# Load cloud optics (this object is relatively small and will be serialized to workers)\n",
    "cloud_optics_lw = rrtmgp_cloud_optics.load_cloud_optics(\n",
    "    cloud_optics_file=CloudOpticsFiles.LW_BND\n",
    ")\n",
    "\n",
    "# Define the function to be applied to each chunk\n",
    "\n",
    "\n",
    "def process_chunk(atm_chunk):\n",
    "    from pyrte_rrtmgp import rrtmgp_cloud_optics\n",
    "\n",
    "    tau_chunk_ds = cloud_optics_lw.compute_cloud_optics(\n",
    "        atm_chunk, problem_type=\"absorption\", add_to_input=False\n",
    "    )\n",
    "    # Assuming the relevant variable is the first one if it's a Dataset, or just use the DataArray\n",
    "    if isinstance(tau_chunk_ds, xr.Dataset):\n",
    "        # Infer the name of the output variable if possible, or assume a default/first one\n",
    "        output_var_name = list(tau_chunk_ds.data_vars.keys())[\n",
    "            0\n",
    "        ]  # Adjust if needed\n",
    "        tau_chunk = tau_chunk_ds[output_var_name]\n",
    "    else:\n",
    "        tau_chunk = tau_chunk_ds\n",
    "\n",
    "    tau_chunk[\"lons\"] = atm_chunk[\"lons\"]\n",
    "    return tau_chunk\n",
    "\n",
    "\n",
    "# Determine dimensions, coordinates, shape, and chunks for the template\n",
    "kept_coords = [coord for coord in atmosphere.coords.keys() if coord]\n",
    "template_coords = {\n",
    "    coord: atmosphere.coords[coord]\n",
    "    for coord in kept_coords\n",
    "    if coord in atmosphere.coords\n",
    "}\n",
    "kept_dims = [dim for dim in atmosphere.dims if dim in kept_coords]\n",
    "\n",
    "# Create a template array like lwp but without the level dimension\n",
    "# Create a template array like lwp but with an additional dimension of size 16\n",
    "base_array = atmosphere[\"lwp\"]\n",
    "# Create a new dask array with the additional dimension\n",
    "dask_data = da.full(\n",
    "    shape=(*base_array.shape, 16),\n",
    "    fill_value=np.nan,\n",
    "    chunks=(*base_array.chunks, (16,)),\n",
    "    dtype=base_array.dtype,\n",
    ")\n",
    "\n",
    "# coords = {**base_array.coords, 'bnd': cloud_optics_lw[\"nband\"].values}\n",
    "# Convert to xarray DataArray\n",
    "dask_data = xr.DataArray(\n",
    "    data=dask_data,\n",
    "    dims=list(base_array.dims) + [\"bnd\"],\n",
    "    coords=base_array.coords,\n",
    ")\n",
    "\n",
    "dim_order = (\"layer\", \"bnd\", \"time\", \"nf\", \"Ydim\", \"Xdim\")\n",
    "dask_data = dask_data.transpose(*dim_order)\n",
    "\n",
    "# Create template DataArray with dask array\n",
    "template_agg = xr.DataArray(\n",
    "    data=dask_data,\n",
    "    dims=dask_data.dims,\n",
    "    coords=dict(dask_data.coords),\n",
    "    name=\"aggregated_tau\",\n",
    ")\n",
    "\n",
    "# --- Apply the function chunk-wise using map_blocks ---\n",
    "tau_agg = xr.map_blocks(process_chunk, atmosphere, template=template_agg)\n",
    "\n",
    "print(\"Computing and saving result to NetCDF...\")\n",
    "with ProgressBar():\n",
    "    tau_agg.to_netcdf(\"computed_tau.nc\", compute=False)\n",
    "    tau_agg.compute(scheduler=\"multiprocessing\")\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rte_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
